import os
import sys
import json
import time
import traceback
from kafka import KafkaConsumer
from pymongo import MongoClient
from pymongo.errors import ConnectionFailure, ServerSelectionTimeoutError
from pymongo import ReplaceOne
from dotenv import load_dotenv
from monitoring import get_monitoring_service

# Load environment variables
load_dotenv()

# Service de monitoring
monitoring = get_monitoring_service()

# ================================
# ğŸ”§ Configuration
# ================================

# Configuration Kafka
KAFKA_BOOTSTRAP_SERVERS = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'localhost:9092')
KAFKA_TOPIC = os.getenv('KAFKA_TOPIC', 'polymarket-events')
KAFKA_GROUP_ID = os.getenv('KAFKA_GROUP_ID', 'polymarket-mongo-consumer')

# Configuration MongoDB
MONGO_URI = os.getenv('MONGO_URI')
MONGO_DB_NAME = os.getenv('DB2', 'polymarket')
MONGO_COLLECTION_NAME = os.getenv('MONGO_COLLECTION', 'polymarket')

# Taille du batch pour l'insertion MongoDB
BATCH_SIZE = int(os.getenv('BATCH_SIZE', '100'))


def connect_mongodb():
    """
    Connexion Ã  MongoDB Atlas
    
    Retourne:
        - MongoClient si succÃ¨s
        - None en cas d'erreur
    """
    try:
        if not MONGO_URI:
            print("âŒ Error: MONGO_URI not found in .env file")
            return None
        
        print("ğŸ”„ Connecting to MongoDB Atlas...")
        client = MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)
        
        # Test de la connexion
        client.admin.command('ping')
        print("âœ… Successfully connected to MongoDB Atlas!")
        
        return client
        
    except (ConnectionFailure, ServerSelectionTimeoutError) as e:
        print(f"âŒ Connection error: {e}")
        return None
    except Exception as e:
        print(f"âŒ Unexpected error: {e}")
        return None


def ensure_unique_index(collection):
    """
    CrÃ©e un index unique sur le champ 'id' pour Ã©viter les doublons
    
    Args:
        collection: collection MongoDB
    """
    try:
        # CrÃ©er un index unique sur le champ 'id'
        collection.create_index('id', unique=True)
        print("âœ… Index unique crÃ©Ã© sur le champ 'id'")
    except Exception as e:
        # L'index existe dÃ©jÃ  ou erreur
        print(f"â„¹ï¸  Index 'id' : {e}")


def create_kafka_consumer():
    """
    CrÃ©e un consommateur Kafka.
    
    Retourne :
        - instance KafkaConsumer si OK
        - None en cas d'erreur
    """
    try:
        print("\nğŸ”„ CrÃ©ation du consommateur Kafka...")
        print(f"   - Bootstrap servers : {KAFKA_BOOTSTRAP_SERVERS}")
        print(f"   - Topic : {KAFKA_TOPIC}")
        print(f"   - Group ID : {KAFKA_GROUP_ID}")

        consumer = KafkaConsumer(
            KAFKA_TOPIC,
            bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
            group_id=KAFKA_GROUP_ID,
            # DÃ©marrer au dÃ©but si nouveau consumer
            auto_offset_reset='earliest',
            # DÃ©sÃ©rialisation JSON des messages
            value_deserializer=lambda m: json.loads(m.decode('utf-8')),
            # Commit automatique des offsets
            enable_auto_commit=True,
            auto_commit_interval_ms=1000,
            # Timeout: arrÃªter aprÃ¨s 30s sans nouveaux messages (mode batch pour DAG)
            consumer_timeout_ms=30000  # 30 secondes d'inactivitÃ© = arrÃªt
        )

        print("âœ… Consommateur Kafka crÃ©Ã© avec succÃ¨s !")
        return consumer

    except Exception as e:
        print(f"âŒ Erreur lors de la crÃ©ation du consommateur Kafka : {e}")
        return None


def insert_batch_to_mongodb(collection, batch):
    """
    InsÃ¨re ou met Ã  jour un batch de documents dans MongoDB (Ã©vite les doublons via le champ 'id')
    
    Args:
        collection: collection MongoDB
        batch: liste de documents Ã  insÃ©rer
    
    Returns:
        Nombre de documents insÃ©rÃ©s ou mis Ã  jour
    """
    try:
        if batch:
            start_time = time.time()
            
            # Utiliser bulk_write avec ReplaceOne pour Ã©viter les doublons
            # Si le document existe (mÃªme 'id'), il est remplacÃ©, sinon insÃ©rÃ©
            operations = [
                ReplaceOne(
                    filter={'id': doc['id']},
                    replacement=doc,
                    upsert=True
                )
                for doc in batch if 'id' in doc
            ]
            
            if operations:
                result = collection.bulk_write(operations, ordered=False)
                duration_ms = int((time.time() - start_time) * 1000)
                
                # Nombre d'insertions + mises Ã  jour
                total_count = result.upserted_count + result.modified_count
                
                print(f"   âœ“ TraitÃ© : {total_count} documents ({result.upserted_count} nouveaux, {result.modified_count} mis Ã  jour) en {duration_ms}ms")
                
                # Log vers PostgreSQL
                monitoring.log_mongodb_stats(
                    collection_name=collection.name,
                    document_count=collection.count_documents({}),
                    insert_count=result.upserted_count,
                    insert_duration_ms=duration_ms
                )
                
                return total_count
            else:
                print("   âš ï¸  Aucun document avec un champ 'id' valide")
                return 0
        return 0
    except Exception as e:
        print(f"   âŒ Erreur lors de l'insertion : {e}")
        monitoring.log_error(
            source='consumer',
            error_type='mongodb_insert_error',
            error_message=str(e),
            stack_trace=traceback.format_exc()
        )
        return 0


def consume_and_insert(consumer, collection, run_id=None):
    """
    Consomme les messages de Kafka et les insÃ¨re dans MongoDB par batch
    
    Args:
        consumer: instance de KafkaConsumer
        collection: collection MongoDB
        run_id: ID du run pour le monitoring
    """
    print("\nğŸ“¨ DÃ©marrage de la consommation des messages Kafka...")
    print(f"   - Taille du batch : {BATCH_SIZE}")
    print(f"   - Collection MongoDB : {MONGO_DB_NAME}.{MONGO_COLLECTION_NAME}")
    print("\n   â³ En attente de messages... (Ctrl+C pour arrÃªter)\n")
    
    batch = []
    total_inserted = 0
    message_count = 0
    
    try:
        for message in consumer:
            # RÃ©cupÃ©ration des donnÃ©es du message
            data = message.value
            batch.append(data)
            message_count += 1
            
            # Insertion par batch
            if len(batch) >= BATCH_SIZE:
                inserted = insert_batch_to_mongodb(collection, batch)
                total_inserted += inserted
                
                # Log mÃ©triques Kafka
                monitoring.log_kafka_metrics(
                    topic=KAFKA_TOPIC,
                    messages_count=message_count,
                    partition=message.partition,
                    offset=message.offset,
                    consumer_group=KAFKA_GROUP_ID
                )
                
                batch = []
                message_count = 0
                print(f"   ğŸ“Š Total insÃ©rÃ© jusqu'Ã  maintenant : {total_inserted} documents\n")
    
    except StopIteration:
        # Timeout atteint (30s sans nouveaux messages) - comportement normal
        print("\nâ±ï¸  Timeout atteint : plus de messages disponibles")
        
        # InsÃ©rer le dernier batch s'il n'est pas vide
        if batch:
            print(f"\nğŸ’¾ Insertion du dernier batch ({len(batch)} documents)...")
            inserted = insert_batch_to_mongodb(collection, batch)
            total_inserted += inserted
        
        print(f"\nâœ… Total de documents traitÃ©s : {total_inserted}")
        print("ğŸ›‘ ArrÃªt du consommateur...")
    
    except KeyboardInterrupt:
        print("\n\nâš ï¸  Interruption par l'utilisateur...")
        
        # InsÃ©rer le dernier batch s'il n'est pas vide
        if batch:
            print(f"\nğŸ’¾ Insertion du dernier batch ({len(batch)} documents)...")
            inserted = insert_batch_to_mongodb(collection, batch)
            total_inserted += inserted
        
        print(f"\nâœ… Total de documents insÃ©rÃ©s : {total_inserted}")
        print("ğŸ›‘ ArrÃªt du consommateur...")
    
    except Exception as e:
        print(f"\nâŒ Erreur lors de la consommation : {e}")
        
        monitoring.log_error(
            source='consumer',
            error_type='consumption_error',
            error_message=str(e),
            stack_trace=traceback.format_exc()
        )
        
        # InsÃ©rer le dernier batch en cas d'erreur
        if batch:
            print(f"\nğŸ’¾ Tentative d'insertion du dernier batch...")
            inserted = insert_batch_to_mongodb(collection, batch)
            total_inserted += inserted
    
    # Mettre Ã  jour le monitoring
    if run_id:
        monitoring.log_pipeline_end(run_id, 'success', total_inserted)
    
    return total_inserted


def main():
    """Fonction principale - Consomme depuis Kafka et insÃ¨re dans MongoDB"""
    print("=" * 60)
    print("    Polymarket Data Consumer (Kafka â†’ MongoDB)")
    print("=" * 60)
    
    # ================================
    # 1) Connexion Ã  MongoDB
    # ================================
    client = connect_mongodb()
    if not client:
        print("\nâŒ Impossible de se connecter Ã  MongoDB. ArrÃªt du script.")
        sys.exit(1)
    
    # RÃ©cupÃ©ration de la collection
    db = client[MONGO_DB_NAME]
    collection = db[MONGO_COLLECTION_NAME]
    
    # CrÃ©er un index unique sur le champ 'id' pour Ã©viter les doublons
    ensure_unique_index(collection)
    
    print(f"\nğŸ“Š Database: {MONGO_DB_NAME}")
    print(f"ğŸ“Š Collection: {MONGO_COLLECTION_NAME}")
    print(f"ğŸ“Š Documents existants: {collection.count_documents({})}")
    
    # ================================
    # 2) CrÃ©ation du consommateur Kafka
    # ================================
    consumer = create_kafka_consumer()
    if not consumer:
        print("\nâŒ Impossible de crÃ©er le consommateur Kafka. ArrÃªt du script.")
        client.close()
        sys.exit(1)
    
    # ================================
    # 3) Consommation et insertion
    # ================================
    
    # DÃ©marrer le monitoring
    run_id = monitoring.log_pipeline_start(
        run_type='consumer',
        metadata={
            'kafka_topic': KAFKA_TOPIC,
            'kafka_group': KAFKA_GROUP_ID,
            'mongodb_collection': f"{MONGO_DB_NAME}.{MONGO_COLLECTION_NAME}",
            'batch_size': BATCH_SIZE
        }
    )
    
    try:
        total = consume_and_insert(consumer, collection, run_id)
        print(f"\nğŸ‰ Processus terminÃ©! Total de documents insÃ©rÃ©s : {total}")
    except Exception as e:
        print(f"\nâŒ Erreur fatale : {e}")
        if run_id:
            monitoring.log_pipeline_end(run_id, 'failed', 0, str(e))
    finally:
        # ================================
        # 4) Nettoyage
        # ================================
        print("\nğŸ§¹ Nettoyage des ressources...")
        
        if consumer:
            try:
                consumer.close()
                print("   âœ“ Consommateur Kafka fermÃ©")
            except Exception:
                pass
        
        if client:
            try:
                client.close()
                print("   âœ“ Connexion MongoDB fermÃ©e")
            except Exception:
                pass
        
        print("\nğŸ‘‹ Au revoir!")


if __name__ == "__main__":
    main()
