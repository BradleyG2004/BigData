import os
import sys
import json
import time
import traceback
from kafka import KafkaConsumer
from pymongo import MongoClient
from pymongo.errors import ConnectionFailure, ServerSelectionTimeoutError
from pymongo import ReplaceOne
from dotenv import load_dotenv
from monitoring import get_monitoring_service

# Load environment variables
load_dotenv()

# Service de monitoring
monitoring = get_monitoring_service()

# ================================
# üîß Configuration
# ================================

# Configuration Kafka
KAFKA_BOOTSTRAP_SERVERS = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'localhost:9092')
KAFKA_TOPIC = os.getenv('KAFKA_TOPIC', 'polymarket-events')
KAFKA_GROUP_ID = os.getenv('KAFKA_GROUP_ID', 'polymarket-mongo-consumer')

# Configuration MongoDB
MONGO_URI = os.getenv('MONGO_URI')
MONGO_DB_NAME = os.getenv('DB2', 'polymarket')
MONGO_COLLECTION_NAME = os.getenv('MONGO_COLLECTION', 'polymarket')

# Taille du batch pour l'insertion MongoDB
BATCH_SIZE = int(os.getenv('BATCH_SIZE', '100'))


def connect_mongodb():
    """
    Connexion √† MongoDB Atlas
    
    Retourne:
        - MongoClient si succ√®s
        - None en cas d'erreur
    """
    try:
        if not MONGO_URI:
            print("‚ùå Error: MONGO_URI not found in .env file")
            return None
        
        print("üîÑ Connecting to MongoDB Atlas...")
        client = MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)
        
        # Test de la connexion
        client.admin.command('ping')
        print("‚úÖ Successfully connected to MongoDB Atlas!")
        
        return client
        
    except (ConnectionFailure, ServerSelectionTimeoutError) as e:
        print(f"‚ùå Connection error: {e}")
        return None
    except Exception as e:
        print(f"‚ùå Unexpected error: {e}")
        return None


def ensure_unique_index(collection):
    """
    Cr√©e un index unique sur le champ 'id' pour √©viter les doublons
    
    Args:
        collection: collection MongoDB
    """
    try:
        # Cr√©er un index unique sur le champ 'id'
        collection.create_index('id', unique=True)
        print("‚úÖ Index unique cr√©√© sur le champ 'id'")
    except Exception as e:
        # L'index existe d√©j√† ou erreur
        print(f"‚ÑπÔ∏è  Index 'id' : {e}")


def create_kafka_consumer():
    """
    Cr√©e un consommateur Kafka.
    
    Retourne :
        - instance KafkaConsumer si OK
        - None en cas d'erreur
    """
    try:
        print("\nüîÑ Cr√©ation du consommateur Kafka...")
        print(f"   - Bootstrap servers : {KAFKA_BOOTSTRAP_SERVERS}")
        print(f"   - Topic : {KAFKA_TOPIC}")
        print(f"   - Group ID : {KAFKA_GROUP_ID}")

        consumer = KafkaConsumer(
            KAFKA_TOPIC,
            bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
            group_id=KAFKA_GROUP_ID,
            # D√©marrer au d√©but si nouveau consumer
            auto_offset_reset='earliest',
            # D√©s√©rialisation JSON des messages
            value_deserializer=lambda m: json.loads(m.decode('utf-8')),
            # Commit automatique des offsets
            enable_auto_commit=True,
            auto_commit_interval_ms=1000
        )

        print("‚úÖ Consommateur Kafka cr√©√© avec succ√®s !")
        return consumer

    except Exception as e:
        print(f"‚ùå Erreur lors de la cr√©ation du consommateur Kafka : {e}")
        return None


def insert_batch_to_mongodb(collection, batch):
    """
    Ins√®re ou met √† jour un batch de documents dans MongoDB (√©vite les doublons via le champ 'id')
    
    Args:
        collection: collection MongoDB
        batch: liste de documents √† ins√©rer
    
    Returns:
        Nombre de documents ins√©r√©s ou mis √† jour
    """
    try:
        if batch:
            start_time = time.time()
            
            # Utiliser bulk_write avec ReplaceOne pour √©viter les doublons
            # Si le document existe (m√™me 'id'), il est remplac√©, sinon ins√©r√©
            operations = [
                ReplaceOne(
                    filter={'id': doc['id']},
                    replacement=doc,
                    upsert=True
                )
                for doc in batch if 'id' in doc
            ]
            
            if operations:
                result = collection.bulk_write(operations, ordered=False)
                duration_ms = int((time.time() - start_time) * 1000)
                
                # Nombre d'insertions + mises √† jour
                total_count = result.upserted_count + result.modified_count
                
                print(f"   ‚úì Trait√© : {total_count} documents ({result.upserted_count} nouveaux, {result.modified_count} mis √† jour) en {duration_ms}ms")
                
                # Log vers PostgreSQL
                monitoring.log_mongodb_stats(
                    collection_name=collection.name,
                    document_count=collection.count_documents({}),
                    insert_count=result.upserted_count,
                    insert_duration_ms=duration_ms
                )
                
                return total_count
            else:
                print("   ‚ö†Ô∏è  Aucun document avec un champ 'id' valide")
                return 0
        return 0
    except Exception as e:
        print(f"   ‚ùå Erreur lors de l'insertion : {e}")
        monitoring.log_error(
            source='consumer',
            error_type='mongodb_insert_error',
            error_message=str(e),
            stack_trace=traceback.format_exc()
        )
        return 0


def consume_and_insert(consumer, collection, run_id=None):
    """
    Consomme les messages de Kafka et les ins√®re dans MongoDB par batch
    
    Args:
        consumer: instance de KafkaConsumer
        collection: collection MongoDB
        run_id: ID du run pour le monitoring
    """
    print("\nüì® D√©marrage de la consommation des messages Kafka...")
    print(f"   - Taille du batch : {BATCH_SIZE}")
    print(f"   - Collection MongoDB : {MONGO_DB_NAME}.{MONGO_COLLECTION_NAME}")
    print("\n   ‚è≥ En attente de messages... (Ctrl+C pour arr√™ter)\n")
    
    batch = []
    total_inserted = 0
    message_count = 0
    
    try:
        for message in consumer:
            # R√©cup√©ration des donn√©es du message
            data = message.value
            batch.append(data)
            message_count += 1
            
            # Insertion par batch
            if len(batch) >= BATCH_SIZE:
                inserted = insert_batch_to_mongodb(collection, batch)
                total_inserted += inserted
                
                # Log m√©triques Kafka
                monitoring.log_kafka_metrics(
                    topic=KAFKA_TOPIC,
                    partition=message.partition,
                    offset=message.offset,
                    messages_count=message_count,
                    consumer_group=KAFKA_GROUP_ID
                )
                
                batch = []
                message_count = 0
                print(f"   üìä Total ins√©r√© jusqu'√† maintenant : {total_inserted} documents\n")
    
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Interruption par l'utilisateur...")
        
        # Ins√©rer le dernier batch s'il n'est pas vide
        if batch:
            print(f"\nüíæ Insertion du dernier batch ({len(batch)} documents)...")
            inserted = insert_batch_to_mongodb(collection, batch)
            total_inserted += inserted
        
        print(f"\n‚úÖ Total de documents ins√©r√©s : {total_inserted}")
        print("üõë Arr√™t du consommateur...")
    
    except Exception as e:
        print(f"\n‚ùå Erreur lors de la consommation : {e}")
        
        monitoring.log_error(
            source='consumer',
            error_type='consumption_error',
            error_message=str(e),
            stack_trace=traceback.format_exc()
        )
        
        # Ins√©rer le dernier batch en cas d'erreur
        if batch:
            print(f"\nüíæ Tentative d'insertion du dernier batch...")
            inserted = insert_batch_to_mongodb(collection, batch)
            total_inserted += inserted
    
    # Mettre √† jour le monitoring
    if run_id:
        monitoring.log_pipeline_end(run_id, 'success', total_inserted)
    
    return total_inserted


def main():
    """Fonction principale - Consomme depuis Kafka et ins√®re dans MongoDB"""
    print("=" * 60)
    print("    Polymarket Data Consumer (Kafka ‚Üí MongoDB)")
    print("=" * 60)
    
    # ================================
    # 1) Connexion √† MongoDB
    # ================================
    client = connect_mongodb()
    if not client:
        print("\n‚ùå Impossible de se connecter √† MongoDB. Arr√™t du script.")
        sys.exit(1)
    
    # R√©cup√©ration de la collection
    db = client[MONGO_DB_NAME]
    collection = db[MONGO_COLLECTION_NAME]
    
    # Cr√©er un index unique sur le champ 'id' pour √©viter les doublons
    ensure_unique_index(collection)
    
    print(f"\nüìä Database: {MONGO_DB_NAME}")
    print(f"üìä Collection: {MONGO_COLLECTION_NAME}")
    print(f"üìä Documents existants: {collection.count_documents({})}")
    
    # ================================
    # 2) Cr√©ation du consommateur Kafka
    # ================================
    consumer = create_kafka_consumer()
    if not consumer:
        print("\n‚ùå Impossible de cr√©er le consommateur Kafka. Arr√™t du script.")
        client.close()
        sys.exit(1)
    
    # ================================
    # 3) Consommation et insertion
    # ================================
    
    # D√©marrer le monitoring
    run_id = monitoring.log_pipeline_start(
        run_type='consumer',
        metadata={
            'kafka_topic': KAFKA_TOPIC,
            'kafka_group': KAFKA_GROUP_ID,
            'mongodb_collection': f"{MONGO_DB_NAME}.{MONGO_COLLECTION_NAME}",
            'batch_size': BATCH_SIZE
        }
    )
    
    try:
        total = consume_and_insert(consumer, collection, run_id)
        print(f"\nüéâ Processus termin√©! Total de documents ins√©r√©s : {total}")
    except Exception as e:
        print(f"\n‚ùå Erreur fatale : {e}")
        if run_id:
            monitoring.log_pipeline_end(run_id, 'failed', 0, str(e))
    finally:
        # ================================
        # 4) Nettoyage
        # ================================
        print("\nüßπ Nettoyage des ressources...")
        
        if consumer:
            try:
                consumer.close()
                print("   ‚úì Consommateur Kafka ferm√©")
            except Exception:
                pass
        
        if client:
            try:
                client.close()
                print("   ‚úì Connexion MongoDB ferm√©e")
            except Exception:
                pass
        
        print("\nüëã Au revoir!")


if __name__ == "__main__":
    main()
