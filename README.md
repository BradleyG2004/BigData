# ğŸ¯ Guide de DÃ©marrage Complet - Pipeline Polymarket

## Architecture ComplÃ¨te

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         POLYMARKET PIPELINE                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“¡ API Polymarket
    â†“
ğŸ”¥ Kafka (Topic: polymarket-events)
    â†“
ğŸ—„ï¸ MongoDB (Collection: polymarket) [RAW DATA]
    â†“
ğŸ§¹ Cleaning Process
    â†“
ğŸ—„ï¸ MongoDB (Collection: cleaned) [CLEANED DATA]
    â†“
ğŸ—„ï¸ PostgreSQL (Table: polymarket_cleaned) [STRUCTURED DATA]
    â†“
ğŸ“Š Grafana Dashboards [VISUALIZATION & COMPARISON]
    â†‘
ğŸ”¥ Spark Processing (Analysis)
```

## ğŸš€ DÃ©marrage Rapide

### 1. Configuration Initiale

```powershell
# Cloner ou naviguer vers le dossier du projet
cd "C:\Users\Bradlley GANGNOU\OneDrive\Desktop\ArchBigDatA"

# CrÃ©er le fichier .env avec vos credentials MongoDB
# Ã‰diter .env et remplacer MONGO_URI par votre vraie URI MongoDB Atlas
```

### 2. DÃ©marrer tous les services

```powershell
# DÃ©marrer l'infrastructure complÃ¨te
docker-compose up -d

# VÃ©rifier que tous les containers sont dÃ©marrÃ©s
docker-compose ps
```

### 3. VÃ©rifier les Services

| Service | URL | Credentials |
|---------|-----|-------------|
| ğŸŒ¬ï¸ Airflow | http://localhost:8081 | admin / admin |
| ğŸ“Š Grafana | http://localhost:3000 | admin / admin |
| ğŸ”¥ Spark Master | http://localhost:8082 | - |
| ğŸ—„ï¸ PostgreSQL | localhost:5433 | polymarket / polymarket123 |
| ğŸ§© Kafka | localhost:9092 | - |

### 4. Lancer le Pipeline

#### Option A: Via Airflow (RecommandÃ©)

1. Ouvrir http://localhost:8081
2. Se connecter (admin/admin)
3. Activer le DAG `polymarket_data_pipeline`
4. Cliquer sur "Trigger DAG" pour le lancer manuellement

Le pipeline s'exÃ©cutera automatiquement toutes les heures.

#### Option B: Scripts manuels

```powershell
# 1. RÃ©cupÃ©rer les donnÃ©es de l'API et envoyer Ã  Kafka
python producer.py

# 2. Consommer Kafka et insÃ©rer dans MongoDB
python consumer.py

# 3. Nettoyer les donnÃ©es MongoDB
python CleaningPolymarket.py

# 4. Charger dans PostgreSQL
python mongo_to_postgres.py
```

### 5. Visualiser avec Grafana

1. Ouvrir http://localhost:3000
2. Se connecter (admin/admin)
3. Aller dans **Dashboards**
4. SÃ©lectionner:
   - **Polymarket - Cleaned Data Analysis** (donnÃ©es PostgreSQL)
   - **Polymarket - Cleaned vs Raw Data Comparison** (comparaison)

## ğŸ“¦ Structure du Projet

```
ArchBigDatA/
â”œâ”€â”€ ğŸ“„ Docker-compose.yaml           # Orchestration des services
â”œâ”€â”€ ğŸ“„ .env                           # Variables d'environnement
â”œâ”€â”€ ğŸ“„ requirements.txt               # DÃ©pendances Python
â”‚
â”œâ”€â”€ ğŸ“ dags/                          # DAGs Airflow
â”‚   â””â”€â”€ polymarket_pipeline_dag.py   # Pipeline principal
â”‚
â”œâ”€â”€ ğŸ“ grafana/                       # Configuration Grafana
â”‚   â”œâ”€â”€ provisioning/
â”‚   â”‚   â”œâ”€â”€ datasources/
â”‚   â”‚   â”‚   â””â”€â”€ datasources.yml      # PostgreSQL & MongoDB
â”‚   â”‚   â””â”€â”€ dashboards/
â”‚   â”‚       â””â”€â”€ dashboards.yml
â”‚   â””â”€â”€ dashboards/
â”‚       â”œâ”€â”€ polymarket-cleaned-dashboard.json
â”‚       â””â”€â”€ polymarket-comparison-dashboard.json
â”‚
â”œâ”€â”€ ğŸ“ postgres-init/                 # Scripts SQL PostgreSQL
â”‚   â”œâ”€â”€ 01-init.sql                  # Tables de monitoring
â”‚   â””â”€â”€ 02-polymarket-schema.sql     # SchÃ©ma Polymarket
â”‚
â”œâ”€â”€ ğŸ“ spark-apps/                    # Applications Spark
â”‚   â””â”€â”€ spark_consumer.py
â”‚
â”œâ”€â”€ ğŸ producer.py                    # Producteur Kafka
â”œâ”€â”€ ğŸ consumer.py                    # Consommateur Kafka
â”œâ”€â”€ ğŸ CleaningPolymarket.py         # Nettoyage des donnÃ©es
â”œâ”€â”€ ğŸ mongo_to_postgres.py          # Transfert MongoDB â†’ PostgreSQL
â”œâ”€â”€ ğŸ monitoring_mongo.py           # Service de monitoring
â”‚
â””â”€â”€ ğŸ“š Documentation/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ POSTGRES_README.md
    â””â”€â”€ GRAFANA_README.md
```

## ğŸ”„ Flux de DonnÃ©es DÃ©taillÃ©

### Ã‰tape 1: Collecte (API â†’ Kafka)
- **Script**: `producer.py` ou DAG task `fetch_api_send_kafka`
- **Source**: https://gamma-api.polymarket.com/events
- **Destination**: Topic Kafka `polymarket-events`
- **FrÃ©quence**: Toutes les heures (via Airflow)

### Ã‰tape 2: Ingestion (Kafka â†’ MongoDB Raw)
- **Script**: `consumer.py` ou DAG task `consume_kafka_insert_mongo`
- **Source**: Topic Kafka `polymarket-events`
- **Destination**: MongoDB `polymarket.polymarket`
- **Type**: DonnÃ©es brutes, non filtrÃ©es

### Ã‰tape 3: Nettoyage (MongoDB Raw â†’ MongoDB Cleaned)
- **Script**: `CleaningPolymarket.py` ou DAG task `clean_polymarket_data`
- **Source**: MongoDB `polymarket.polymarket`
- **Destination**: MongoDB `polymarket.cleaned`
- **Actions**:
  - âœ… Filtrer: image, icon, seriesSlug, resolutionSource non vides
  - âœ… Supprimer 25+ champs inutiles
  - âœ… Conserver uniquement les donnÃ©es qualitatives

### Ã‰tape 4: Structuration (MongoDB Cleaned â†’ PostgreSQL)
- **Script**: `mongo_to_postgres.py` ou DAG task `load_to_postgres`
- **Source**: MongoDB `polymarket.cleaned`
- **Destination**: PostgreSQL `polymarket.polymarket_cleaned`
- **Avantages**:
  - ğŸ” RequÃªtes SQL performantes
  - ğŸ“Š Jointures et agrÃ©gations avancÃ©es
  - ğŸ¯ Indexation optimisÃ©e

### Ã‰tape 5: Visualisation (PostgreSQL â†’ Grafana)
- **Dashboards**: Comparaison cleaned vs raw
- **MÃ©triques**: QualitÃ©, complÃ©tude, distribution
- **Refresh**: 30s - 1m

### Ã‰tape 6: Analyse (Spark Processing)
- **Script**: `spark_consumer.py` ou DAG task `spark_processing`
- **Analyses**: Machine Learning, prÃ©dictions, tendances

## ğŸ›ï¸ Commandes Utiles

### Docker

```powershell
# DÃ©marrer tous les services
docker-compose up -d

# DÃ©marrer un service spÃ©cifique
docker-compose up -d grafana

# ArrÃªter tous les services
docker-compose down

# ArrÃªter et supprimer les volumes
docker-compose down -v

# Voir les logs d'un service
docker-compose logs -f grafana

# RedÃ©marrer un service
docker-compose restart airflow-webserver

# Voir l'Ã©tat des services
docker-compose ps
```

### PostgreSQL

```powershell
# Se connecter Ã  PostgreSQL
docker exec -it postgres-polymarket psql -U polymarket -d polymarket

# Ou depuis Windows (si psql installÃ©)
psql -h localhost -p 5433 -U polymarket -d polymarket

# RequÃªtes utiles
SELECT COUNT(*) FROM polymarket_cleaned;
SELECT * FROM polymarket_active_events LIMIT 10;
SELECT * FROM polymarket_stats_by_category;
```

### MongoDB

```powershell
# VÃ©rifier le nombre de documents
# Via Python
python -c "from pymongo import MongoClient; import os; from dotenv import load_dotenv; load_dotenv(); client = MongoClient(os.getenv('MONGO_URI')); print('Raw:', client['polymarket']['polymarket'].count_documents({})); print('Cleaned:', client['polymarket']['cleaned'].count_documents({}))"
```

### Kafka

```powershell
# Lister les topics (depuis le container)
docker exec broker kafka-topics.sh --bootstrap-server localhost:9092 --list

# Voir les messages d'un topic
docker exec broker kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic polymarket-events --from-beginning --max-messages 10
```

## ğŸ“Š Monitoring

### 1. Pipeline Airflow

- **URL**: http://localhost:8081
- **DAG**: `polymarket_data_pipeline`
- **Monitoring**: Graph View, Task Duration, Logs

### 2. Grafana Dashboards

- **URL**: http://localhost:3000
- **Dashboards**: Cleaned Analysis, Comparison
- **Metrics**: Count, Quality, Distribution

### 3. PostgreSQL Monitoring

```sql
-- Taille de la base
SELECT pg_size_pretty(pg_database_size('polymarket'));

-- ActivitÃ© rÃ©cente
SELECT * FROM pipeline_runs ORDER BY start_time DESC LIMIT 10;

-- MÃ©triques Kafka
SELECT * FROM kafka_metrics ORDER BY timestamp DESC LIMIT 10;

-- Logs d'erreurs
SELECT * FROM error_logs ORDER BY timestamp DESC LIMIT 10;
```

## ğŸ› DÃ©pannage

### ProblÃ¨me: Airflow ne dÃ©marre pas

```powershell
# VÃ©rifier les logs
docker-compose logs airflow-init
docker-compose logs airflow-webserver

# RÃ©initialiser Airflow
docker-compose down
docker volume rm archbigdata_postgres-db-volume
docker-compose up -d
```

### ProblÃ¨me: Grafana ne trouve pas PostgreSQL

```powershell
# VÃ©rifier que PostgreSQL est dÃ©marrÃ©
docker-compose ps postgres-polymarket

# Tester la connexion
docker exec grafana ping -c 3 postgres-polymarket

# VÃ©rifier les datasources
docker exec grafana cat /etc/grafana/provisioning/datasources/datasources.yml
```

### ProblÃ¨me: DonnÃ©es non transfÃ©rÃ©es vers PostgreSQL

```powershell
# 1. VÃ©rifier MongoDB cleaned
python -c "from pymongo import MongoClient; import os; from dotenv import load_dotenv; load_dotenv(); print(MongoClient(os.getenv('MONGO_URI'))['polymarket']['cleaned'].count_documents({}))"

# 2. ExÃ©cuter manuellement le transfert
python mongo_to_postgres.py

# 3. VÃ©rifier PostgreSQL
docker exec postgres-polymarket psql -U polymarket -d polymarket -c "SELECT COUNT(*) FROM polymarket_cleaned;"
```

### ProblÃ¨me: Kafka ne reÃ§oit pas de messages

```powershell
# VÃ©rifier que Kafka est prÃªt
docker-compose logs broker

# Tester avec le producteur
python producer.py

# VÃ©rifier les messages
docker exec broker kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic polymarket-events --max-messages 5
```

## ğŸ” SÃ©curitÃ©

### Credentials par dÃ©faut (Ã  changer en production)

```env
# Airflow
AIRFLOW_USER=admin
AIRFLOW_PASSWORD=admin

# Grafana
GRAFANA_ADMIN_USER=admin
GRAFANA_ADMIN_PASSWORD=admin

# PostgreSQL
POSTGRES_USER=polymarket
POSTGRES_PASSWORD=polymarket123

# MongoDB
MONGO_URI=mongodb+srv://votre_vrai_uri
```

