# ğŸ¯ Guide de DÃ©marrage Complet - Pipeline Polymarket

## Architecture ComplÃ¨te

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    POLYMARKET PIPELINE v2.0                         â”‚
â”‚              Spark Streaming pour le nettoyage temps rÃ©el          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                    ğŸ“¡ API Polymarket
                           â†“
                    ğŸš€ Producer.py
                           â†“
              ğŸ”¥ Kafka (polymarket-events)
                      â†™        â†˜
                     â†™          â†˜
         [RAW PATH]              [CLEANED PATH]
              â†“                       â†“
    ğŸ‘¨â€ğŸ’» Consumer.py          âš¡ Spark Consumer
              â†“                  (Filtrage + Nettoyage)
    ğŸ—„ï¸ MongoDB                        â†“
    polymarket.polymarket    ğŸ—„ï¸ PostgreSQL
    [RAW DATA]              polymarket_cleaned
         â†“                    [CLEANED DATA]
         â†“                         â†“
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ ğŸ“Š Grafana â†â”€â”€â”˜
                  [COMPARISON]
```

### ğŸ”‘ Points ClÃ©s de l'Architecture

- **2 chemins parallÃ¨les** depuis Kafka
- **MongoDB**: Stockage des donnÃ©es brutes (PRE-Spark)
- **Spark Streaming**: Nettoyage et transformation en temps rÃ©el
- **PostgreSQL**: Stockage des donnÃ©es nettoyÃ©es (POST-Spark)
- **Grafana**: Comparaison RAW vs CLEANED

## ğŸš€ DÃ©marrage Rapide

### 1. Configuration Initiale

```powershell
# Cloner ou naviguer vers le dossier du projet
cd "C:\Users\Bradlley GANGNOU\OneDrive\Desktop\ArchBigDatA"

# CrÃ©er le fichier .env avec vos credentials MongoDB
# Ã‰diter .env et remplacer MONGO_URI par votre vraie URI MongoDB Atlas
```

### 2. DÃ©marrer tous les services

```powershell
# DÃ©marrer l'infrastructure complÃ¨te
docker-compose up -d

# VÃ©rifier que tous les containers sont dÃ©marrÃ©s
docker-compose ps
```

### 3. VÃ©rifier les Services

| Service | URL | Credentials |
|---------|-----|-------------|
| ğŸŒ¬ï¸ Airflow | http://localhost:8081 | admin / admin |
| ğŸ“Š Grafana | http://localhost:3000 | admin / admin |
| ğŸ”¥ Spark Master | http://localhost:8082 | - |
| ğŸ—„ï¸ PostgreSQL | localhost:5433 | polymarket / polymarket123 |
| ğŸ§© Kafka | localhost:9092 | - |

### 4. Lancer le Pipeline

#### Option A: Via Airflow (RecommandÃ©)

1. Ouvrir http://localhost:8081
2. Se connecter (admin/admin)
3. Activer le DAG `polymarket_data_pipeline`
4. Cliquer sur "Trigger DAG" pour le lancer manuellement

Le pipeline s'exÃ©cutera automatiquement toutes les heures.

#### Option B: Scripts manuels

```powershell
# 1. RÃ©cupÃ©rer les donnÃ©es de l'API et envoyer Ã  Kafka
python producer.py

# 2. Consommer Kafka et insÃ©rer dans MongoDB (RAW)
python kafka/actors/consumer.py

# 3. Traiter avec Spark et insÃ©rer dans PostgreSQL (CLEANED)
docker exec spark-master spark-submit \
  --master spark://spark-master:7077 \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.postgresql:postgresql:42.7.1 \
  /opt/spark-apps/spark_consumer.py
```

**âš ï¸ Note**: Les scripts `CleaningPolymarket.py` et `mongo_to_postgres.py` sont **obsolÃ¨tes** et remplacÃ©s par le Spark Consumer.

### 5. Visualiser avec Grafana

1. Ouvrir http://localhost:3000
2. Se connecter (admin/admin)
3. Aller dans **Dashboards**
4. SÃ©lectionner:
   - **Polymarket - Cleaned Data Analysis** (donnÃ©es PostgreSQL)
   - **Polymarket - Cleaned vs Raw Data Comparison** (comparaison)

## ğŸ“¦ Structure du Projet

```
ArchBigDatA/
â”œâ”€â”€ ğŸ“„ Docker-compose.yaml           # Orchestration des services
â”œâ”€â”€ ğŸ“„ .env                           # Variables d'environnement
â”œâ”€â”€ ğŸ“„ requirements.txt               # DÃ©pendances Python
â”‚
â”œâ”€â”€ ğŸ“ dags/                          # DAGs Airflow
â”‚   â””â”€â”€ polymarket_pipeline_dag.py   # Pipeline principal
â”‚
â”œâ”€â”€ ğŸ“ grafana/                       # Configuration Grafana
â”‚   â”œâ”€â”€ provisioning/
â”‚   â”‚   â”œâ”€â”€ datasources/
â”‚   â”‚   â”‚   â””â”€â”€ datasources.yml      # PostgreSQL & MongoDB
â”‚   â”‚   â””â”€â”€ dashboards/
â”‚   â”‚       â””â”€â”€ dashboards.yml
â”‚   â””â”€â”€ dashboards/
â”‚       â”œâ”€â”€ polymarket-cleaned-dashboard.json
â”‚       â””â”€â”€ polymarket-comparison-dashboard.json
â”‚
â”œâ”€â”€ ğŸ“ postgres-init/                 # Scripts SQL PostgreSQL
â”‚   â”œâ”€â”€ 01-init.sql                  # Tables de monitoring
â”‚   â””â”€â”€ 02-polymarket-schema.sql     # SchÃ©ma Polymarket
â”‚
â”œâ”€â”€ ğŸ“ spark-apps/                    # Applications Spark
â”‚   â””â”€â”€ spark_consumer.py            # âš¡ Kafka â†’ Cleaning â†’ PostgreSQL
â”‚
â”œâ”€â”€ ğŸ“ kafka/actors/                  # Scripts Kafka
â”‚   â”œâ”€â”€ producer.py                  # Producteur Kafka
â”‚   â””â”€â”€ consumer.py                  # Consommateur Kafka â†’ MongoDB
â”‚
â”œâ”€â”€ ğŸ CleaningPolymarket.py         # âš ï¸ OBSOLÃˆTE (remplacÃ© par Spark)
â”œâ”€â”€ ğŸ mongo_to_postgres.py          # âš ï¸ OBSOLÃˆTE (remplacÃ© par Spark)
â”œâ”€â”€ ğŸ collect_mongo_stats.py        # Collecte stats MongoDB
â”œâ”€â”€ ğŸ monitoring.py                 # Service de monitoring
â”‚
â””â”€â”€ ğŸ“š Documentation/
    â”œâ”€â”€ README.md
    â”œâ”€â”€ POSTGRES_README.md
    â””â”€â”€ GRAFANA_README.md
```

## ğŸ”„ Flux de DonnÃ©es DÃ©taillÃ©

### Ã‰tape 1: Collecte (API â†’ Kafka)
- **Script**: `kafka/actors/producer.py` ou DAG task `fetch_api_send_kafka`
- **Source**: https://gamma-api.polymarket.com/events
- **Destination**: Topic Kafka `polymarket-events`
- **FrÃ©quence**: Toutes les heures (via Airflow) ou toutes les 5 min (via `kafka_producer_consumer` DAG)

### Ã‰tape 2A: Ingestion RAW (Kafka â†’ MongoDB)
- **Script**: `kafka/actors/consumer.py` ou DAG task `consume_kafka_insert_mongo`
- **Source**: Topic Kafka `polymarket-events`
- **Destination**: MongoDB `Polymarket.polymarket`
- **Type**: DonnÃ©es brutes, non filtrÃ©es
- **Usage**: Comparaison dans Grafana (donnÃ©es RAW)
- **DÃ©duplication**: Index unique sur le champ `id`

### Ã‰tape 2B: Traitement CLEANED (Kafka â†’ Spark â†’ PostgreSQL)
- **Script**: `spark-apps/spark_consumer.py` ou DAG task `spark_processing`
- **Source**: Topic Kafka `polymarket-events` (streaming)
- **Destination**: PostgreSQL `polymarket.polymarket_cleaned`
- **Traitement Spark**:
  - âœ… **Filtrage**: image, icon, seriesSlug, resolutionSource non vides
  - âœ… **Suppression**: 28 champs inutiles (liquidity, archived, new, etc.)
  - âœ… **Transformation**: Renommage colonnes (idâ†’mongo_id, conditionIdâ†’condition_id)
  - âœ… **Conversion dates**: Timestamps vers format PostgreSQL
  - âœ… **JSON preservation**: outcomes et outcomePrices en JSONB
- **Mode**: Streaming temps rÃ©el (pas de batch)
- **Avantages**:
  - ğŸ” RequÃªtes SQL performantes
  - ğŸ“Š Jointures et agrÃ©gations avancÃ©es
  - ğŸ¯ Indexation optimisÃ©e (9 index dont unique sur mongo_id)
  - âš¡ Traitement distribuÃ© scalable

### Ã‰tape 3: Visualisation (MongoDB + PostgreSQL â†’ Grafana)
- **Datasource RAW**: Table `mongodb_stats` (stats collectÃ©es depuis MongoDB)
- **Datasource CLEANED**: Table PostgreSQL `polymarket_cleaned`
- **Dashboards**: 
  - Cleaned Data Analysis (mÃ©triques PostgreSQL)
  - Comparison RAW vs CLEANED (impact du filtrage Spark)
- **MÃ©triques**: Count, qualitÃ©, complÃ©tude, distribution par catÃ©gorie
- **Refresh**: 30s - 1m

### ğŸ“Š Comparaison Architecture v1 vs v2

| Aspect | v1 (Ancien) | v2 (Actuel) |
|--------|-------------|-------------|
| **Nettoyage** | Python batch (CleaningPolymarket.py) | Spark Streaming temps rÃ©el |
| **Transfert** | Python batch (mongo_to_postgres.py) | Spark JDBC direct |
| **MongoDB cleaned** | Existe (intermÃ©diaire) | âš ï¸ N'existe plus |
| **Latence** | Batch horaire | Streaming continu |
| **ScalabilitÃ©** | LimitÃ©e | DistribuÃ©e (Spark) |
| **Ã‰tapes** | 4 scripts sÃ©quentiels | 2 chemins parallÃ¨les |

## ğŸ›ï¸ Commandes Utiles

### Docker

```powershell
# DÃ©marrer tous les services
docker-compose up -d

# DÃ©marrer un service spÃ©cifique
docker-compose up -d grafana

# ArrÃªter tous les services
docker-compose down

# ArrÃªter et supprimer les volumes
docker-compose down -v

# Voir les logs d'un service
docker-compose logs -f grafana

# RedÃ©marrer un service
docker-compose restart airflow-webserver

# Voir l'Ã©tat des services
docker-compose ps
```

### PostgreSQL

```powershell
# Se connecter Ã  PostgreSQL
docker exec -it postgres-polymarket psql -U polymarket -d polymarket

# Ou depuis Windows (si psql installÃ©)
psql -h localhost -p 5433 -U polymarket -d polymarket

# RequÃªtes utiles
SELECT COUNT(*) FROM polymarket_cleaned;
SELECT * FROM polymarket_active_events LIMIT 10;
SELECT * FROM polymarket_stats_by_category;
```

### MongoDB

```powershell
# VÃ©rifier le nombre de documents RAW
# Via Python
python -c "from pymongo import MongoClient; import os; from dotenv import load_dotenv; load_dotenv(); client = MongoClient(os.getenv('MONGO_URI')); print('Raw:', client['Polymarket']['polymarket'].count_documents({}))"

# Note: MongoDB 'cleaned' collection n'existe plus (remplacÃ©e par Spark â†’ PostgreSQL direct)
```

### Kafka

```powershell
# Lister les topics (depuis le container)
docker exec broker kafka-topics.sh --bootstrap-server localhost:9092 --list

# Voir les messages d'un topic
docker exec broker kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic polymarket-events --from-beginning --max-messages 10
```

## ğŸ“Š Monitoring

### 1. Pipeline Airflow

- **URL**: http://localhost:8081
- **DAG**: `polymarket_data_pipeline`
- **Monitoring**: Graph View, Task Duration, Logs

### 2. Grafana Dashboards

- **URL**: http://localhost:3000
- **Dashboards**: Cleaned Analysis, Comparison
- **Metrics**: Count, Quality, Distribution

### 3. PostgreSQL Monitoring

```sql
-- Taille de la base
SELECT pg_size_pretty(pg_database_size('polymarket'));

-- ActivitÃ© rÃ©cente
SELECT * FROM pipeline_runs ORDER BY start_time DESC LIMIT 10;

-- MÃ©triques Kafka
SELECT * FROM kafka_metrics ORDER BY timestamp DESC LIMIT 10;

-- Logs d'erreurs
SELECT * FROM error_logs ORDER BY timestamp DESC LIMIT 10;
```

## ğŸ› DÃ©pannage

### ProblÃ¨me: Airflow ne dÃ©marre pas

```powershell
# VÃ©rifier les logs
docker-compose logs airflow-init
docker-compose logs airflow-webserver

# RÃ©initialiser Airflow
docker-compose down
docker volume rm archbigdata_postgres-db-volume
docker-compose up -d
```

### ProblÃ¨me: Grafana ne trouve pas PostgreSQL

```powershell
# VÃ©rifier que PostgreSQL est dÃ©marrÃ©
docker-compose ps postgres-polymarket

# Tester la connexion
docker exec grafana ping -c 3 postgres-polymarket

# VÃ©rifier les datasources
docker exec grafana cat /etc/grafana/provisioning/datasources/datasources.yml
```

### ProblÃ¨me: DonnÃ©es non transfÃ©rÃ©es vers PostgreSQL

```powershell
# 1. VÃ©rifier que Kafka reÃ§oit les messages
docker exec broker kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic polymarket-events --max-messages 5

# 2. VÃ©rifier les logs Spark
docker-compose logs spark-master
docker-compose logs spark-worker-1

# 3. VÃ©rifier PostgreSQL
docker exec postgres-polymarket psql -U polymarket -d polymarket -c "SELECT COUNT(*) FROM polymarket_cleaned;"

# 4. Relancer le Spark Consumer manuellement
docker exec spark-master spark-submit \
  --master spark://spark-master:7077 \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.postgresql:postgresql:42.7.1 \
  /opt/spark-apps/spark_consumer.py
```

### ProblÃ¨me: Kafka ne reÃ§oit pas de messages

```powershell
# VÃ©rifier que Kafka est prÃªt
docker-compose logs broker

# Tester avec le producteur
python producer.py

# VÃ©rifier les messages
docker exec broker kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic polymarket-events --max-messages 5
```

## ğŸ” SÃ©curitÃ©

### Credentials par dÃ©faut (Ã  changer en production)

```env
# Airflow
AIRFLOW_USER=admin
AIRFLOW_PASSWORD=admin

# Grafana
GRAFANA_ADMIN_USER=admin
GRAFANA_ADMIN_PASSWORD=admin

# PostgreSQL
POSTGRES_USER=polymarket
POSTGRES_PASSWORD=polymarket123

# MongoDB
MONGO_URI=mongodb+srv://votre_vrai_uri
```

