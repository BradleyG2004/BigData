# ğŸ“Š Polymarket Data Pipeline - Orchestration Airflow

> Pipeline de donnÃ©es orchestrÃ© par **Apache Airflow** pour collecter, traiter et monitorer les Ã©vÃ©nements Polymarket.

## ğŸ—ï¸ Architecture avec Orchestration

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Airflow   â”‚ (Orchestrateur)
                    â”‚  Scheduler  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚              â”‚              â”‚
            â–¼              â–¼              â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  Task 1 â”‚    â”‚  Task 2 â”‚   â”‚  Task 3 â”‚
      â”‚ APIâ†’Kafkaâ”‚â”€â”€â”€â–¶â”‚Kafkaâ†’Mongoâ”€â”€â–¶â”‚  Spark  â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚              â”‚              â”‚
            â–¼              â–¼              â–¼
      [Kafka Topic]  [MongoDB Atlas]  [Analytics]
                           â–²
                           â”‚
                    [Monitoring MongoDB]
```

## ğŸš€ DÃ©marrage Rapide

```bash
# 1. Configuration
cp .env.example .env
# Ã‰diter .env : remplir MONGO_URI

# 2. CrÃ©er dossiers Airflow
mkdir -p dags logs plugins config

# 3. DÃ©marrer
docker-compose up -d

# 4. AccÃ©der Ã  Airflow
# http://localhost:8081
# Username: admin / Password: admin

# 5. Activer le DAG
# Cliquer sur le toggle dans l'UI Airflow
```

## ğŸ“Š Workflow du DAG

```
1. check_kafka_ready (vÃ©rifie Kafka disponible)
      â†“
2. fetch_api_send_kafka (API â†’ Kafka)
      â†“
3. consume_kafka_insert_mongo (Kafka â†’ MongoDB)
      â†“  
4. spark_processing (traitement analytics)
```

### Schedule

Par dÃ©faut : **@hourly** (toutes les heures)

Modifiable dans `dags/polymarket_pipeline_dag.py` :
```python
schedule_interval='@hourly'  # ou @daily, @weekly, cron syntax, etc.
```

## ğŸ” Monitoring

### Airflow UI
- **URL** : http://localhost:8081
- **Graph View** : Visualisation du workflow
- **Logs** : Logs dÃ©taillÃ©s de chaque task
- **Stats** : Performance et historique

### MongoDB Atlas - `polymarket_monitoring`

Collections :
- `pipeline_metrics` : ExÃ©cutions des pipelines
- `batch_inserts` : Performance des insertions
- `kafka_metrics` : MÃ©triques Kafka
- `error_logs` : Erreurs capturÃ©es

### Spark UI
- **URL** : http://localhost:8080
- Jobs et workers

## ğŸ› ï¸ Troubleshooting

### DAG n'apparaÃ®t pas

```bash
# VÃ©rifier les logs
docker logs airflow-scheduler

# Lister les DAGs
docker exec airflow-scheduler airflow dags list
```

### Erreur Kafka

```bash
# VÃ©rifier Kafka
docker ps | grep broker
docker logs broker

# Tester la connexion
docker exec airflow-webserver python -c "
from kafka import KafkaProducer
p = KafkaProducer(bootstrap_servers='broker:9092')
print('âœ… Kafka OK')
p.close()
"
```

### Erreur MongoDB

VÃ©rifier :
1. `MONGO_URI` dans `.env`
2. IP whitelisted dans MongoDB Atlas
3. Credentials corrects

### RÃ©initialisation complÃ¨te

```bash
docker-compose down -v
rm -rf logs/*
docker-compose up -d
```

## ğŸ“ Variables ClÃ©s (.env)

```env
# Requis
POLYMARKET_API_URL=https://gamma-api.polymarket.com/events
MONGO_URI=mongodb+srv://user:pass@cluster.mongodb.net/

# Optionnel (valeurs par dÃ©faut OK)
KAFKA_BOOTSTRAP_SERVERS=broker:9092
KAFKA_TOPIC=polymarket-events
DB2=polymarket_db
MONITORING_DB=polymarket_monitoring
BATCH_SIZE=100
```

## ğŸ“ Pourquoi Airflow ?

### Avantages

âœ… **Orchestration** : EnchaÃ®nement automatique des tÃ¢ches  
âœ… **Scheduling** : ExÃ©cution programmÃ©e (hourly, daily, etc.)  
âœ… **Retry Logic** : Relance automatique en cas d'Ã©chec  
âœ… **Monitoring** : UI complÃ¨te pour suivre tout  
âœ… **Alerting** : Notifications en cas de problÃ¨me  
âœ… **ScalabilitÃ©** : Facile d'ajouter des tasks  

### Cas d'usage

- âœ… Pipeline batch rÃ©gulier (hourly, daily)
- âœ… DÃ©pendances entre tasks
- âœ… Besoin de retry automatique
- âœ… Ã‰quipe qui a besoin de visibilitÃ©

## ğŸ”„ Ã‰volution du Projet

### Version 1.0 (Sans orchestration)
- Scripts Python indÃ©pendants
- Consumer en boucle infinie
- Lancement manuel

### Version 2.0 (Avec Airflow) â† Actuel
- Orchestration Airflow
- Consumer dÃ©clenchÃ© par task
- Monitoring MongoDB intÃ©grÃ©
- Scheduling automatique

## ğŸ“š Documentation

- [README_FULL.md](README.md) - Documentation complÃ¨te
- [dags/polymarket_pipeline_dag.py](dags/polymarket_pipeline_dag.py) - Code du DAG

---

**Quick Start** : `docker-compose up -d` â†’ http://localhost:8081 â†’ Activer le DAG ğŸš€
