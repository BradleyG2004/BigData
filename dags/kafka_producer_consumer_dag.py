"""
DAG Airflow - Kafka Producer & Consumer
========================================

Ce DAG g√®re uniquement la partie Kafka du pipeline:
1. V√©rifier que Kafka est pr√™t
2. Fetch API ‚Üí Envoyer √† Kafka (Producer)
3. Consommer depuis Kafka ‚Üí Ins√©rer dans MongoDB (Consumer)

Schedule: Toutes les 5 minutes (automatique)
Auteur: Data Pipeline Team
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
import subprocess
import time
import os
import sys

# Configuration par d√©faut
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=2),
    'execution_timeout': timedelta(minutes=30),
}


def check_kafka_availability():
    """
    V√©rifie que Kafka est disponible et pr√™t √† recevoir des messages.
    """
    print("üîç V√©rification de la disponibilit√© de Kafka...")
    
    kafka_broker = os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'kafka:29092')
    max_retries = 10
    retry_interval = 5
    
    for attempt in range(1, max_retries + 1):
        try:
            print(f"Tentative {attempt}/{max_retries}...")
            
            # Utilise kafka-broker-api-versions pour tester la connexion
            result = subprocess.run(
                ['kafka-broker-api-versions', '--bootstrap-server', kafka_broker],
                capture_output=True,
                text=True,
                timeout=10
            )
            
            if result.returncode == 0:
                print("‚úÖ Kafka est disponible et op√©rationnel")
                return True
            else:
                print(f"‚ö†Ô∏è Kafka pas encore pr√™t: {result.stderr}")
                
        except subprocess.TimeoutExpired:
            print(f"‚è±Ô∏è Timeout lors de la tentative {attempt}")
        except Exception as e:
            print(f"‚ùå Erreur lors de la v√©rification: {e}")
        
        if attempt < max_retries:
            print(f"‚è≥ Attente de {retry_interval}s avant nouvelle tentative...")
            time.sleep(retry_interval)
    
    raise Exception("‚ùå Kafka n'est pas disponible apr√®s plusieurs tentatives")


def run_kafka_producer():
    """
    Ex√©cute le producer Kafka qui r√©cup√®re les donn√©es de l'API Polymarket
    et les envoie √† Kafka.
    """
    print("="*70)
    print("üöÄ D√âMARRAGE DU PRODUCER KAFKA")
    print("="*70)
    print(f"üìÖ Timestamp: {datetime.now()}")
    print(f"üîó Kafka Broker: {os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'kafka:29092')}")
    print(f"üìù Topic: {os.getenv('KAFKA_TOPIC', 'polymarket-events')}")
    print("="*70)
    
    # Chemin vers le script producer
    producer_script = '/opt/airflow/producer.py'
    
    if not os.path.exists(producer_script):
        raise FileNotFoundError(f"‚ùå Script producer introuvable: {producer_script}")
    
    print(f"üìÇ Ex√©cution du script: {producer_script}")
    
    try:
        # Ex√©cute le producer
        result = subprocess.run(
            ['python', producer_script],
            capture_output=True,
            text=True,
            timeout=600,  # 10 minutes max
            cwd='/opt/airflow'
        )
        
        print("\n" + "="*70)
        print("üì§ OUTPUT DU PRODUCER")
        print("="*70)
        print(result.stdout)
        
        if result.stderr:
            print("\n" + "="*70)
            print("‚ö†Ô∏è WARNINGS/ERRORS")
            print("="*70)
            print(result.stderr)
        
        if result.returncode != 0:
            raise Exception(f"‚ùå Le producer a √©chou√© avec le code: {result.returncode}")
        
        print("\n" + "="*70)
        print("‚úÖ PRODUCER TERMIN√â AVEC SUCC√àS")
        print("="*70)
        
    except subprocess.TimeoutExpired:
        raise Exception("‚ùå Le producer a d√©pass√© le timeout de 10 minutes")
    except Exception as e:
        raise Exception(f"‚ùå Erreur lors de l'ex√©cution du producer: {e}")


def run_kafka_consumer():
    """
    Ex√©cute le consumer Kafka qui r√©cup√®re les messages depuis Kafka
    et les ins√®re dans MongoDB.
    """
    print("="*70)
    print("üöÄ D√âMARRAGE DU CONSUMER KAFKA")
    print("="*70)
    print(f"üìÖ Timestamp: {datetime.now()}")
    print(f"üîó Kafka Broker: {os.getenv('KAFKA_BOOTSTRAP_SERVERS', 'kafka:29092')}")
    print(f"üìù Topic: {os.getenv('KAFKA_TOPIC', 'polymarket-events')}")
    print(f"üíæ MongoDB: {os.getenv('MONGO_DB', 'polymarket')}")
    print("="*70)
    
    # Chemin vers le script consumer
    consumer_script = '/opt/airflow/consumer.py'
    
    if not os.path.exists(consumer_script):
        raise FileNotFoundError(f"‚ùå Script consumer introuvable: {consumer_script}")
    
    print(f"üìÇ Ex√©cution du script: {consumer_script}")
    
    try:
        # Ex√©cute le consumer
        result = subprocess.run(
            ['python', consumer_script],
            capture_output=True,
            text=True,
            timeout=600,  # 10 minutes max
            cwd='/opt/airflow'
        )
        
        print("\n" + "="*70)
        print("üì• OUTPUT DU CONSUMER")
        print("="*70)
        print(result.stdout)
        
        if result.stderr:
            print("\n" + "="*70)
            print("‚ö†Ô∏è WARNINGS/ERRORS")
            print("="*70)
            print(result.stderr)
        
        if result.returncode != 0:
            raise Exception(f"‚ùå Le consumer a √©chou√© avec le code: {result.returncode}")
        
        print("\n" + "="*70)
        print("‚úÖ CONSUMER TERMIN√â AVEC SUCC√àS")
        print("="*70)
        
    except subprocess.TimeoutExpired:
        raise Exception("‚ùå Le consumer a d√©pass√© le timeout de 10 minutes")
    except Exception as e:
        raise Exception(f"‚ùå Erreur lors de l'ex√©cution du consumer: {e}")


def print_summary():
    """
    Affiche un r√©sum√© de l'ex√©cution du pipeline Kafka.
    """
    print("\n" + "="*70)
    print("üìä R√âSUM√â DU PIPELINE KAFKA")
    print("="*70)
    print("‚úÖ √âtape 1: V√©rification Kafka - SUCC√àS")
    print("‚úÖ √âtape 2: Producer Kafka (API ‚Üí Kafka) - SUCC√àS")
    print("‚úÖ √âtape 3: Consumer Kafka (Kafka ‚Üí MongoDB) - SUCC√àS")
    print("="*70)
    print(f"üéâ Pipeline Kafka ex√©cut√© avec succ√®s √† {datetime.now()}")
    print("="*70)
    print("\nüí° Prochaines √©tapes:")
    print("   1. V√©rifier les donn√©es dans MongoDB collection 'polymarket'")
    print("   2. Le DAG se r√©-ex√©cutera automatiquement dans 5 minutes")
    print("   3. Consulter les dashboards Grafana pour voir les stats")
    print("="*70)


# D√©finition du DAG
with DAG(
    dag_id='kafka_producer_consumer',
    default_args=default_args,
    description='Pipeline Kafka: API ‚Üí Producer ‚Üí Kafka ‚Üí Consumer ‚Üí MongoDB',
    schedule_interval=timedelta(minutes=5),  # Ex√©cution automatique toutes les 5 minutes
    start_date=datetime(2026, 2, 1),
    catchup=False,
    tags=['kafka', 'polymarket', 'producer', 'consumer', 'automatic'],
    max_active_runs=1,  # Une seule ex√©cution √† la fois
) as dag:
    
    # Documentation du DAG
    dag.doc_md = """
    # üöÄ DAG Kafka Producer & Consumer
    
    ## üìã Description
    Ce DAG g√®re uniquement la collecte de donn√©es via Kafka:
    - **Producer**: R√©cup√®re les donn√©es de l'API Polymarket et les envoie √† Kafka
    - **Consumer**: Lit les messages Kafka et les ins√®re dans MongoDB
    
    ## üéØ Objectif
    Alimenter la collection MongoDB 'polymarket' avec les donn√©es brutes de l'API.
    
    ## ‚öôÔ∏è √âtapes
    1. **check_kafka**: V√©rifie la disponibilit√© de Kafka
    2. **run_producer**: API Polymarket ‚Üí Kafka
    3. **run_consumer**: Kafka ‚Üí MongoDB (collection 'polymarket')
    4. **summary**: Affiche le r√©sum√©
    
    ## üîß Schedule
    - **Automatique toutes les 5 minutes**
    - Peut aussi √™tre d√©clench√© manuellement via l'UI Airflow
    
    ## üìä R√©sultat
    - Donn√©es brutes dans MongoDB collection: `polymarket`
    - Collecte continue et mise √† jour r√©guli√®re
    
    ## üîó DAGs li√©s
    - `polymarket_data_pipeline`: Pipeline complet (cleaning + PostgreSQL + Spark)
    """
    
    # T√¢che 1: V√©rifier Kafka
    check_kafka = PythonOperator(
        task_id='check_kafka_ready',
        python_callable=check_kafka_availability,
        doc_md="""
        ### V√©rification Kafka
        
        V√©rifie que le broker Kafka est disponible avant de lancer le producer.
        - Max retries: 10
        - Interval: 5 secondes
        """
    )
    
    # T√¢che 2: Producer Kafka
    producer = PythonOperator(
        task_id='kafka_producer',
        python_callable=run_kafka_producer,
        doc_md="""
        ### Producer Kafka
        
        R√©cup√®re les donn√©es de l'API Polymarket et les envoie √† Kafka.
        
        **Script**: `producer.py`
        **Topic**: `polymarket-events`
        **Timeout**: 10 minutes
        """
    )
    
    # T√¢che 3: Consumer Kafka
    consumer = PythonOperator(
        task_id='kafka_consumer',
        python_callable=run_kafka_consumer,
        doc_md="""
        ### Consumer Kafka
        
        Consomme les messages depuis Kafka et les ins√®re dans MongoDB.
        
        **Script**: `consumer.py`
        **Topic**: `polymarket-events`
        **MongoDB Collection**: `polymarket` (raw data)
        **Timeout**: 10 minutes
        """
    )
    
    # T√¢che 4: R√©sum√©
    summary = PythonOperator(
        task_id='print_summary',
        python_callable=print_summary,
        doc_md="""
        ### R√©sum√©
        
        Affiche un r√©sum√© de l'ex√©cution et les prochaines √©tapes.
        """
    )
    
    # D√©finition du workflow
    check_kafka >> producer >> consumer >> summary
