apiVersion: 1

datasources:
  # ================================
  # PostgreSQL - Données CLEANED (POST-Spark)
  # ================================
  # Source: Données nettoyées et transformées par Spark Consumer
  # Pipeline: Kafka → Spark (filtrage + nettoyage) → PostgreSQL
  # Table: polymarket_cleaned
  # Utilisation: Dashboard de comparaison (données cleaned)
  - name: PostgreSQL - Polymarket Cleaned
    type: postgres
    access: proxy
    url: postgres-polymarket:5432
    database: polymarket
    user: polymarket
    secureJsonData:
      password: polymarket123
    jsonData:
      sslmode: disable
      postgresVersion: 1300  # PostgreSQL 13
      timescaledb: false
    isDefault: true
    editable: true
    uid: postgres-cleaned

  # ================================
  # MongoDB - Données RAW (PRE-Spark)
  # ================================
  # Source: Données brutes insérées par consumer.py depuis Kafka
  # Pipeline: Kafka → Consumer Python → MongoDB
  # Collection: Polymarket.polymarket
  # Utilisation: Dashboard de comparaison (données raw)
  #
  # ⚠️ Note: MongoDB datasource doit être configurée manuellement via l'UI Grafana
  # Car les variables d'environnement ne sont pas interpolées dans les fichiers de provisioning
  #
  # Pour ajouter MongoDB manuellement:
  # 1. Aller dans Configuration → Data Sources → Add data source
  # 2. Chercher "MongoDB" et installer le plugin si nécessaire
  # 3. Configurer avec votre MONGO_URI depuis .env
  # 4. Database: Polymarket (avec P majuscule)
  # 5. Collection à monitorer: polymarket (données brutes avant Spark)

