# üóÑÔ∏è PostgreSQL Integration - Stockage des donn√©es nettoy√©es Polymarket

## Vue d'ensemble

Cette int√©gration ajoute un container PostgreSQL d√©di√© au stockage des donn√©es Polymarket nettoy√©es. Apr√®s le nettoyage dans MongoDB (collection `cleaned`), les donn√©es sont automatiquement charg√©es dans PostgreSQL pour des analyses SQL avanc√©es et une meilleure structuration.

## Architecture

```
API Polymarket ‚Üí Kafka ‚Üí MongoDB (raw) ‚Üí Nettoyage ‚Üí MongoDB (cleaned) ‚Üí PostgreSQL
                                                                              ‚Üì
                                                                         Spark Processing
```

## Composants ajout√©s

### 1. Container PostgreSQL (`postgres-polymarket`)

- **Image**: `postgres:13`
- **Port**: `5433` (pour √©viter conflit avec postgres-airflow)
- **Credentials par d√©faut**:
  - User: `polymarket`
  - Password: `polymarket123`
  - Database: `polymarket_db`

### 2. Table principale: `polymarket_cleaned`

Structure de la table avec tous les champs nettoy√©s:

```sql
- mongo_id (VARCHAR) - ID unique de MongoDB
- condition_id (VARCHAR) - ID de condition Polymarket
- question_id (VARCHAR) - ID de la question
- title (TEXT) - Titre de l'√©v√©nement
- description (TEXT) - Description
- category (VARCHAR) - Cat√©gorie
- series_slug (VARCHAR) - Slug de la s√©rie
- image, icon (TEXT) - URLs des images
- outcomes (JSONB) - R√©sultats possibles
- outcome_prices (JSONB) - Prix actuels
- volume, volume_num (NUMERIC) - Volumes
- start_date, end_date (TIMESTAMP) - Dates
- ... et plus
```

### 3. Vues SQL pr√©-cr√©√©es

#### `polymarket_stats_by_category`
Statistiques agr√©g√©es par cat√©gorie:
```sql
SELECT * FROM polymarket_stats_by_category;
```

#### `polymarket_active_events`
√âv√©nements en cours (end_date > NOW()):
```sql
SELECT * FROM polymarket_active_events;
```

#### `polymarket_top_volume`
Top 100 des √©v√©nements par volume:
```sql
SELECT * FROM polymarket_top_volume;
```

### 4. Script Python: `mongo_to_postgres.py`

Script standalone pour transf√©rer manuellement les donn√©es:

```bash
python mongo_to_postgres.py
```

### 5. Task Airflow: `load_to_postgres`

Int√©gr√©e dans le DAG `polymarket_data_pipeline`, cette t√¢che:
1. Se connecte √† MongoDB (collection `cleaned`)
2. Transforme les documents au format PostgreSQL
3. Vide la table PostgreSQL (√©vite doublons)
4. Ins√®re les donn√©es par batch
5. V√©rifie l'int√©grit√© des donn√©es

## Configuration

### Variables d'environnement (.env)

```bash
# PostgreSQL pour donn√©es Polymarket
POSTGRES_HOST=localhost          # Utilisez 'postgres-polymarket' dans Docker
POSTGRES_PORT=5433               # Port expos√© sur l'h√¥te
POSTGRES_USER=polymarket
POSTGRES_PASSWORD=polymarket123
POSTGRES_DB=polymarket_db
```

### Docker Compose

Le container PostgreSQL d√©marre automatiquement avec:

```bash
docker-compose up -d postgres-polymarket
```

## Utilisation

### 1. D√©marrage de l'infrastructure

```bash
# D√©marrer tous les containers
docker-compose up -d

# V√©rifier que PostgreSQL est pr√™t
docker-compose ps postgres-polymarket
docker-compose logs postgres-polymarket
```

### 2. Connexion √† PostgreSQL

#### Depuis votre machine (Windows):

```bash
# Via psql
psql -h localhost -p 5433 -U polymarket -d polymarket_db

# Via pgAdmin ou DBeaver
Host: localhost
Port: 5433
User: polymarket
Password: polymarket123
Database: polymarket_db
```

#### Depuis un container Docker:

```bash
docker exec -it postgres-polymarket psql -U polymarket -d polymarket_db
```

### 3. Ex√©cution manuelle du transfert

```bash
# Assurer que les variables d'environnement sont configur√©es
python mongo_to_postgres.py
```

### 4. Via Airflow DAG

Le DAG `polymarket_data_pipeline` ex√©cute automatiquement:

1. ‚úÖ `check_kafka_ready` - V√©rifier Kafka
2. ‚úÖ `fetch_api_send_kafka` - API ‚Üí Kafka
3. ‚úÖ `consume_kafka_insert_mongo` - Kafka ‚Üí MongoDB (raw)
4. ‚úÖ `clean_polymarket_data` - Nettoyage ‚Üí MongoDB (cleaned)
5. **üÜï `load_to_postgres`** - MongoDB (cleaned) ‚Üí PostgreSQL
6. ‚úÖ `spark_processing` - Traitement Spark

## Requ√™tes SQL utiles

### Compter les documents

```sql
SELECT COUNT(*) FROM polymarket_cleaned;
```

### Top 10 par volume

```sql
SELECT title, category, volume_num, end_date
FROM polymarket_cleaned
ORDER BY volume_num DESC
LIMIT 10;
```

### √âv√©nements par cat√©gorie

```sql
SELECT category, COUNT(*) as total,
       AVG(volume_num) as avg_volume
FROM polymarket_cleaned
GROUP BY category
ORDER BY total DESC;
```

### √âv√©nements se terminant dans les 24h

```sql
SELECT title, category, end_date, volume
FROM polymarket_cleaned
WHERE end_date BETWEEN NOW() AND NOW() + INTERVAL '24 hours'
ORDER BY end_date ASC;
```

### Recherche dans les outcomes (JSONB)

```sql
SELECT title, outcomes
FROM polymarket_cleaned
WHERE outcomes::text ILIKE '%Trump%'
LIMIT 10;
```

## Monitoring et Logs

### Logs du container PostgreSQL

```bash
docker-compose logs -f postgres-polymarket
```

### Logs de la task Airflow

1. Acc√©der √† Airflow UI: http://localhost:8081
2. Aller dans le DAG `polymarket_data_pipeline`
3. Cliquer sur la task `load_to_postgres`
4. Voir les logs dans l'onglet "Logs"

### Statistiques de la base

```sql
-- Taille de la table
SELECT pg_size_pretty(pg_total_relation_size('polymarket_cleaned'));

-- Nombre d'index
SELECT COUNT(*) FROM pg_indexes WHERE tablename = 'polymarket_cleaned';

-- Statistiques de la table
SELECT * FROM pg_stat_user_tables WHERE relname = 'polymarket_cleaned';
```

## D√©pannage

### Le container ne d√©marre pas

```bash
# V√©rifier les logs
docker-compose logs postgres-polymarket

# Recr√©er le container
docker-compose down postgres-polymarket
docker-compose up -d postgres-polymarket
```

### Erreur de connexion

```bash
# V√©rifier que le container est healthy
docker-compose ps

# Tester la connexion depuis le container
docker exec -it postgres-polymarket pg_isready -U polymarket
```

### La table n'existe pas

```bash
# V√©rifier que les scripts d'initialisation ont √©t√© ex√©cut√©s
docker exec -it postgres-polymarket psql -U polymarket -d polymarket_db -c "\dt"

# Si n√©cessaire, r√©ex√©cuter l'initialisation
docker exec -it postgres-polymarket psql -U polymarket -d polymarket_db -f /docker-entrypoint-initdb.d/02-polymarket-schema.sql
```

### Les donn√©es ne sont pas transf√©r√©es

1. V√©rifier que la collection `cleaned` dans MongoDB contient des donn√©es:
   ```python
   from pymongo import MongoClient
   client = MongoClient(MONGO_URI)
   print(client['polymarket_db']['cleaned'].count_documents({}))
   ```

2. Ex√©cuter manuellement le script de transfert:
   ```bash
   python mongo_to_postgres.py
   ```

3. V√©rifier les logs du DAG Airflow

## Maintenance

### Backup de la base

```bash
# Backup complet
docker exec postgres-polymarket pg_dump -U polymarket polymarket_db > backup_$(date +%Y%m%d).sql

# Backup de la table uniquement
docker exec postgres-polymarket pg_dump -U polymarket -t polymarket_cleaned polymarket_db > backup_table_$(date +%Y%m%d).sql
```

### Restauration

```bash
# Restaurer depuis un backup
cat backup_20260209.sql | docker exec -i postgres-polymarket psql -U polymarket polymarket_db
```

### Nettoyage des anciennes donn√©es

```sql
-- Supprimer les √©v√©nements termin√©s depuis plus de 30 jours
DELETE FROM polymarket_cleaned
WHERE end_date < NOW() - INTERVAL '30 days';

-- Vacuum pour r√©cup√©rer l'espace
VACUUM FULL polymarket_cleaned;
```

## Am√©liorations futures

- [ ] Partitionnement de la table par date
- [ ] R√©plication PostgreSQL pour haute disponibilit√©
- [ ] Int√©gration avec TimescaleDB pour s√©ries temporelles
- [ ] Dashboard Grafana connect√© √† PostgreSQL
- [ ] API REST pour interroger PostgreSQL
- [ ] Synchronisation incr√©mentale (au lieu de TRUNCATE)

## Support

Pour des questions ou probl√®mes:
1. V√©rifier les logs: `docker-compose logs -f postgres-polymarket`
2. Consulter la documentation PostgreSQL: https://www.postgresql.org/docs/
3. V√©rifier les variables d'environnement dans `.env`

